---
title: FinalProject
author: SABRY Abdellah/ Nassim Dahmani/ Salem Ben Mabrouk
date: 18/03/2017
output:
  html_document:
    highlight: espresso
    theme: cerulean
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
subtitle: NBA shots stats
---
 
# Description of the dataset
 
We wanted to make this project on an intersting dataset. So we'll analyse a sport dataset.
We went to kaggle.com and we found a dataset that matters us. The NBA shot logs.
We downloaded the csv file.
Each row is a shot of a basketball player during a match.
 
## Context
* 1 GAME_ID 
The ID of the match.
 
* 2 MATCHUP 
Date of the match separated from the two opponents.
 
* 3 LOCATION
A : Match away, we'll transform it in 1 
H : Match at home, we'll transform it in 0
 
* 4 W  
Either the match is won by the basketball player or not.
W : Won, we'll transform it in 1
L : Lost, we'll transform it in 0 
 
* 5 FINAL_MARGIN 
Difference of points between the team of the player and the opponents.
 
* 6 SHOT_NUMBER 
ID of the shot for the player during the match.
 
* 7 PERIOD 
Number of the period.
 
* 8 GAME_CLOCK 
Time remaining before the end of the period.
 
* 9 SHOT_CLOCK  
Time remaining before the shot clock.
 
* 10 DRIBBLES  
Number of dribbles before the shot during this shotclock.
 
* 11 TOUCH_TIME 
The time the player keeped the ball.
 
* 12 SHOT_DIST 
The distance between the player and the basket.
 
* 13 PTS_TYPE 
The number of points corresponding of the emplacement of his shot.
 
* 14 SHOT_RESULT 
Either the shot is missed or made.
 
* 15 CLOSEST_DEFENDER 
The name of the closest defender.
 
* 16 CLOSEST_DEFENDER_PLAYER_ID  
The id of the closest defender.
 
* 17 CLOSE_DEF_DIST 
The distance between the defender and the shooter.
 
* 18 FGM 
Either the shot is missed or made.
0 : Missed 
1 : Made 
 
* 19 PTS 
Number of the points made at the end.
 
* 20 player_name 
The name of the player.
 
* 21 player_id 
The ID of the player.
 
 
 
# General analysis
 In a first time we clean our dataset by setting all the variables numeric
 
```{r}
 
dataset = read.csv("C:\\Users\\Abdel\\Desktop\\ESILV_S8\\Machine Learning\\datasets\\shot_logs.csv")
dataset=dataset[,-c(1,2,5,6,8,14,15,16,19,20,21)]
dataset$LOCATION = ifelse(dataset$LOCATION == "A", 1, 0)
dataset$W = ifelse(dataset$W == "W", 1, 0)
dataset[is.na(dataset)] <- 0
 
```

Then we split the database into two collections, trainig_set and test_set. The training_set will be used to perform the models and the test_set is used to compute the accuracy.
We had tested the correlation between variables in order to choose the variables we'll use in our models.

```{r}
library(caTools) 
set.seed(123)

split = sample.split(dataset$FGM, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
nrow(test_set)
nrow(training_set)



names <- c('LOCATION','W','PERIOD', 'SHOT_CLOCK', 'DRIBBLES','TOUCH_TIME','SHOT_DIST','CLOSE_DEF_DIST', 'PTS_TYPE')
 
for (name in names){
  training_set[, name] = as.numeric(training_set[,name])
  test_set[, name] = as.numeric(test_set[,name])
  dataset[,name] = as.numeric(dataset[,name])
}
dataset$FGM = as.factor(dataset$FGM)
test_set$FGM = as.factor(test_set$FGM)
training_set$FGM = as.factor(training_set$FGM)

```
You can see some graphics that describe and explain our database
```{r}
par(mfrow=c(1,4))
 
boxplot(dataset$SHOT_DIST~ dataset$FGM,varwidth = TRUE, notch = TRUE, outline = TRUE, ylab='Distance of the shot (feet)')
boxplot(dataset$PERIOD~ dataset$FGM,varwidth = TRUE, notch = TRUE, outline = TRUE, ylab='Period of the shot')
boxplot(dataset$SHOT_CLOCK~ dataset$FGM,varwidth = TRUE, notch = TRUE, outline = TRUE, ylab='Shot clock when the shot is made')
boxplot(dataset$DRIBBLES~ dataset$FGM,varwidth = TRUE, notch = TRUE, outline = TRUE, ylab='Number of dribbles before the shot')
par(mfrow=c(1,3))
boxplot(dataset$TOUCH_TIME~ dataset$FGM,varwidth = TRUE, notch = TRUE, outline = TRUE, ylab='Touch time before the shot')
boxplot(dataset$PTS_TYPE~ dataset$FGM,varwidth = TRUE, notch = TRUE, outline = TRUE, ylab='Type of shot')
boxplot(dataset$CLOSE_DEF_DIST~ dataset$FGM,varwidth = TRUE, notch = TRUE, outline = TRUE, ylab='Distance of the closest defenser during the shot')
```
 
 
```{r}
library(ggplot2)

ggplot(data=dataset, aes(dataset$PERIOD, fill = FGM)) + geom_histogram(binwidth=.2, alpha=.5) + xlab("Number of shots") + ylab("Quarter")

ggplot(data=dataset, aes(dataset$SHOT_CLOCK, fill=FGM)) + geom_histogram(binwidth=.2, alpha=.5)+ xlab("Number of shots") + ylab("Shot clock remaining")

ggplot(data=dataset, aes(dataset$DRIBBLES, fill=FGM)) + geom_histogram(binwidth=.2, alpha=.5) + xlab("Number of shots") + ylab("Number of dribbles")

ggplot(data=dataset, aes(dataset$TOUCH_TIME, fill=FGM)) + geom_histogram(binwidth=.2, alpha=.5)+ xlab("Number of shots") + ylab("The time the player keeped it")

ggplot(data=dataset, aes(dataset$SHOT_DIST, fill=FGM)) + geom_histogram(binwidth=.2, alpha=.5)+ xlab("Number of shots") + ylab("The distance of the shot")

ggplot(data=dataset, aes(dataset$PTS_TYPE, fill=FGM)) + geom_histogram(binwidth=.2, alpha=.5)+ xlab("Number of shots") + ylab("Number of points")

ggplot(data=dataset, aes(dataset$CLOSE_DEF_DIST, fill=FGM)) + geom_histogram(binwidth=.2, alpha=.5)+ xlab("Number of shots") + ylab("Distance between the player and the closest defenser")
```
 
# Supervised Machine Learning
 
## Correlation matrix
We compute the correlation matrix to visalize the correlation between the variables.
 
```{r}

corr=training_set
corr[,"FGM"]=as.numeric(corr[,"FGM"])
correlation=round(cor(x=corr,y=corr[,"FGM"]),digits=2)
correlation
library(corrplot)
corrplot(correlation,method="ellipse")
```
 
##Scale the variables
Then we scaled our database in order to have more precise results
```{r}
training_set[,-10]=scale(training_set[,-10])
test_set[,-10]=scale(test_set[,-10])
summary(training_set)
summary(test_set)
```

 
## Generalized linear model (GLM)
 
First I take the variables for which the correlation coefficients are above 0,5
Our variables are discrets so we first use the logistic regression model
Moreover, we began with the glm model because we are predicting a binary outcome from a set of continuous predictor variables. We preferred over discriminant function analysis because of its less restrictive assumptions.
 
 
```{r}

model = glm(FGM ~ CLOSE_DEF_DIST+PTS_TYPE+W,  family="binomial", data = test_set)
summary(model)
```
 
 
```{r}
prediction1= predict(model,type="response",newdata = test_set)
pred1 = ifelse(prediction1 > 0.5, 1, 0)
confusMat1 = table(pred1, test_set$FGM )
confusMat1
truePos = confusMat1[2,2]
trueNeg = confusMat1[1,1]
falsePos = confusMat1[1,2]
falseNeg = confusMat1[2,1]
accuracy = (truePos+trueNeg)/(truePos+trueNeg+falseNeg+falsePos)
accuracy
sensitivity = truePos/truePos+falseNeg
sensitivity
specificity = trueNeg/falsePos+trueNeg
specificity
```
 
But we only had an accuracy of 0.56 so i tried to erase Location and period which are unsignificant but it didn't change anything
 
After observations on this graphic we had seen that the shot distance is really relevant for the shot to be missed or made
 
```{r}
ggplot(data=dataset, aes(x=SHOT_DIST, y=CLOSE_DEF_DIST)) + geom_point(aes(color=factor(FGM)))
```
 
 
So we decided to add SHOT_DIST TO our model after we plot this variables
 
```{r}
model = glm(FGM ~ CLOSE_DEF_DIST+PTS_TYPE+W+SHOT_DIST+PERIOD+LOCATION,family="binomial", data = test_set)
summary(model)
```
 
```{r}
y_prediction.logreg = predict(model,type="response",newdata = test_set)
pred1 = ifelse(y_prediction.logreg > 0.5, 1, 0)
confusMat1 = table(pred1, test_set$FGM )
truePos = confusMat1[2,2]
trueNeg = confusMat1[1,1]
falsePos = confusMat1[1,2]
falseNeg = confusMat1[2,1]
accuracy = (truePos+trueNeg)/(truePos+trueNeg+falseNeg+falsePos)
accuracy
sensitivity = truePos/truePos+falseNeg
sensitivity
specificity = trueNeg/falsePos+trueNeg
specificity
```
 
We finished with an accuracy of 0.60 which is way much better than 0.56
 
Then we'll use the MASS model with the same commands the better accuracy we had was 0.60
 
## Linear discriminant analysis (LDA)
 
```{r}
library(MASS)
classifier.lda <- lda(FGM ~  PTS_TYPE +SHOT_DIST + CLOSE_DEF_DIST + W+ PERIOD, data=training_set)
summary(classifier.lda)
```
 
```{r}
y_prediction_lda=predict(classifier.lda,type="response",newdata=test_set)
 
 
confusion.lda=table(y_prediction_lda$class,test_set$FGM)
accuracy.lda=sum(diag(confusion.lda))/sum(confusion.lda)
accuracy.lda
```
 
 
## Quadratic discriminant analysis (QDA)
 
```{r}
classifier.qda <- qda(FGM ~  PTS_TYPE + CLOSE_DEF_DIST+SHOT_DIST + W+ PERIOD, data=training_set)
summary(classifier.qda)
 
```
 Then we had tested the qda model with the same commands the better accuracy we had was 0.60, because the Quadratic discriminant function does not assume homogeneity of variance-covariance matrices
 
```{r}
y_prediction_qda=predict(classifier.qda,type="response",newdata=test_set)
 
 
confusion.qda=table(y_prediction_qda$class,test_set$FGM)
accuracy.qda=sum(diag(confusion.qda))/sum(confusion.qda)
accuracy.qda
```
  
In order to show the difference between the modelisation we're gonna represent them
 
 
```{r}
library(ROCR)
 
performance_lda <- performance(
  prediction(y_prediction_lda$posterior[,2],test_set$FGM),
  "tpr","fpr")
 
plot(performance_lda,
     main="ROC function of the LDA,QDA and LOGREG",
     col="blue")
 
performance_qda=performance(
  prediction(y_prediction_qda$posterior[,2],test_set$FGM),
  "tpr","fpr")
plot(performance_qda,
     col="red",
     add= TRUE)
 
performance_logreg = performance(
    prediction(y_prediction.logreg,test_set$FGM),
    "tpr","fpr")
 
plot(performance_logreg,
     col="green",
     add= TRUE)
 
abline(a=0, b=1, col="gray")
 
legend(0.80,0.57,c("LDA","QDA","Logreg","y=x"),cex=1,lty=c(1,1,1,1),
       lwd=c(2.5,2.5,2.5,2.5),col=c("blue","red","green","gray"))


#to be sure we'll calculate the auc value, it's stocked in the y.Values of the result of performance (taking the parameter auc)
auc_lda= performance(prediction(y_prediction_lda$posterior[,2],test_set$FGM),"auc")
auc_lda_value = as.numeric(auc_lda@y.values)
 
auc_qda= performance(prediction(y_prediction_qda$posterior[,2],test_set$FGM),"auc")
auc_qda_value = as.numeric(auc_qda@y.values)
 
auc_logreg= performance(prediction(y_prediction.logreg,test_set$FGM),"auc")
auc_logreg_value = as.numeric(auc_logreg@y.values)
 
## adding the values of differents auc on the graph directly
auc_lda_tostring= paste(c("LDA's AUC  = "),round(auc_lda_value,2),sep="")
auc_qda_tostring= paste(c("QDA's AUC  = "),round(auc_qda_value,2),sep="")
auc_logreg_tostring= paste(c("LOGREG's AUC  = "),round(auc_logreg_value,2),sep="")
legend(0.63,0.24,c(auc_lda_tostring,auc_qda_tostring,auc_logreg_tostring),cex=1)
```
As we can see there are very small differencies between the three model but the more accurate we have is the lda model

# Unsupervised Machine Learning 

## Principal component analysis (PCA)
 
```{r}
dataset[,"FGM"]= as.numeric(dataset[,"FGM"])
pca <- princomp(dataset, cor = TRUE)
summary(pca)
plot(pca, type="l")
biplot(pca, cex=.5,xlab="PC1" , ylab="PC2")
plot(x=pca$scores[,1], y=pca$scores[,2],col=dataset$FGM)
```

 

At the beginning we thought that it would be better to split our datas in hierarchical mode, as we can't know the number of clusters we'll find. The dendogram cluster helps to split data in hierarchical way and so to give an idea of the correlation between observations
 
## Dendrogram
 
```{r}
dataset.first=head(dataset,10000);
distance=dist(dataset.first)
treeComp <- hclust(distance, method = "complete")
plot(treeComp)
rect.hclust(treeComp,k = 20,border="red")
 
```
 
After printing the dendogram, we've seen that it is impossible to create cluster like this because of the too big amount of datas
 
Then we decided to use the k-means to find the best clusters for the dataset. For this we used a program that permit us to choose the numbers of clusters to apply k-means. In fact, in order to apply the k-means we need to know the exact numbers of cluster
 
So we create a hierarchical view that permit to group variables, not observations.
Clusters with AU > = 95% are considered to be strongly supported by data;
 
## Cluster dendrogram
 
```{r}
#install.packages("pvclust")
library(pvclust)
dataset.pv=pvclust(dataset.first);
plot(dataset.pv)
pvrect(dataset.pv,alpha = 0.95)
 
```
 
We had verified with the elbow method that if we take 4 clusters if it's a good representation. We see a small ressemblance at the elbow on cluster number 4, that confirms that it's a good cluster choice
 
## K-means
```{r}
mydata <- dataset
within = (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:20) within[i] = sum(kmeans(mydata, centers=i)$withinss)
plot(1:20, within, type="b", xlab="Nb of Clusters",
     ylab="Within groups sum of squares")
 
```
 
We apply the k-means on 4 cluster, we've seen some interesting informations as centroid place of the clusters ans the size of each cluster

```{r}
kmeans.dataset = kmeans(dataset,centers=4)
head(kmeans.dataset$cluster,100)
# centers of each cluster
head(kmeans.dataset$centers,100)
# size of each cluster
kmeans.dataset$size